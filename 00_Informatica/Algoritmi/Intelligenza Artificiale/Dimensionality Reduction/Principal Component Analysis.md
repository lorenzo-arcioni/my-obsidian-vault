# Principal Component Analysis (PCA)

## üìò Definizione
La **Principal Component Analysis (PCA)** √® una tecnica di riduzione della dimensionalit√†, che si basa sulla feature extraction, utilizzata per comprimere i dati preservando la maggior parte delle informazioni rilevanti. Viene ampiamente applicata in machine learning, analisi dei dati e statistica.

L'obiettivo principale della PCA √®:
> Trovare una rappresentazione dello spazio originale dei dati in un sistema di coordinate trasformato, chiamato "componenti principali", che massimizzi la varianza dei dati.

 √à strettamente collegata all'ipotesi dei [[Manifold Hypothesis|Manifold]].

---

## üéØ Obiettivi principali

1. **Riduzione della dimensionalit√†**: Ridurre il numero di variabili per semplificare l'analisi.
2. **Rimozione della collinearit√†**: Ridurre la ridondanza tra le variabili.
3. **Visualizzazione**: Rappresentare dati complessi in 2D o 3D per comprenderne la struttura.

---
## üìä Dataset di partenza

Consideriamo un dataset $D = \{\vec{x}_i\}_{i=1}^N$, con $\vec{x}_i \in \mathbb{R}^d$, dove:
- $N$: numero di osservazioni
- $d$: numero di variabili (dimensioni)

Scritto in forma di matrice, $D$ pu√≤ essere rappresentato come:
$$
D = \begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,d} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,d} \\
\vdots & \vdots & \ddots & \vdots \\
x_{N,1} & x_{N,2} & \cdots & x_{N,d}
\end{bmatrix}
$$

Dove $x_{i,j}$ rappresenta il valore della $j$-esima variabile per il punto $\vec{x}_i$.

---

## üõ†Ô∏è Procedura dettagliata

### 1Ô∏è‚É£ **Standardizzazione del dataset**
Per ogni variabile $j \in \{1, 2, \dots, d\}$, calcoliamo:
1. La media:
   $$
   \mu_j = \frac{1}{N} \sum_{i=1}^N x_{i,j}
   $$
2. La deviazione standard:
   $$
   \sigma_j = \sqrt{\frac{1}{N} \sum_{i=1}^N (x_{i,j} - \mu_j)^2}
   $$

Per ogni punto $\vec{x}_i = [x_{i,1}, x_{i,2}, \dots, x_{i,d}]$, standardizziamo ogni dimensione:
$$
z_{i,j} = \frac{x_{i,j} - \mu_j}{\sigma_j}
$$

Il nuovo dataset standardizzato diventa:
$$
Z = \begin{bmatrix}
z_{1,1} & z_{1,2} & \cdots & z_{1,d} \\
z_{2,1} & z_{2,2} & \cdots & z_{2,d} \\
\vdots & \vdots & \ddots & \vdots \\
z_{N,1} & z_{N,2} & \cdots & z_{N,d}
\end{bmatrix}
$$

---

### 2Ô∏è‚É£ **Calcolo della matrice di covarianza**
La [[Covarianza|matrice di covarianza]] $\Sigma$ misura la relazione lineare tra le variabili del dataset standardizzato $Z$:
$$
\Sigma = \frac{1}{N-1} Z^\top Z
$$
Dove $Z^\top$ √® la trasposta della matrice $Z$.

Ogni elemento $\Sigma_{j,k}$ della matrice di covarianza rappresenta la covarianza tra le variabili $j$ e $k$.

Notiamo inoltre l'applicazione della [[Correzione di Bessel]] nella formula della covarianza.

---

### 3Ô∏è‚É£ **Calcolo di autovalori e autovettori**
Calcoliamo gli **autovalori** $\lambda_1, \lambda_2, \dots, \lambda_d$ e gli **autovettori** $\vec{v}_1, \vec{v}_2, \dots, \vec{v}_d$ della matrice di covarianza $\Sigma$:
$$
\Sigma \vec{v}_j = \lambda_j \vec{v}_j
$$
- Gli autovalori $\lambda_j$ indicano la (quantit√† di) varianza catturata dal componente principale associato. Infatti, se sommiamo tutti gli autovalori, otteniamo la varianza totale.
- Gli autovettori $\vec{v}_j$ definiscono la direzione dei componenti principali.

L'unico caso in cui possiamo ottenere una soluzione non banale dell'equazione (i.e. $\vec v_j = \vec 0$) √® quando la matrice $(\Sigma - \lambda_j I)$ non √® invertibile, infatti se lo fosse
$$\begin{align}
\Sigma \vec{v}_j &= \lambda_j \vec{v}_j\\
\Sigma \vec{v}_j - \lambda_j \vec{v}_j &= \vec 0\\
(\Sigma - \lambda_j I) \vec{v}_j &= \vec 0\\
(\Sigma - \lambda_j I)^{-1} (\Sigma - \lambda_j I) \vec{v}_j &= (\Sigma - \lambda_j I)^{-1} \vec 0\\
\vec v_j &= \vec 0.
\end{align}
$$
avremmo la soluzione banale. Quindi dobbiamo assumere per forza che 
$$
det[(\Sigma - \lambda_j I)] = 0.
$$
Dato che questa equazione √® un polinomio di grado al massimo $d$, pu√≤ avere al massimo $d$ soluzioni, e quindi non pi√π di $d$ autovettori e autovalori.

Una volta trovati gli autovalori, non ci resta altro che sostituirli nell'equazione
$$
(\Sigma - \lambda_j I) \vec{v}_j = \vec 0
$$
per ottenere i rispettivi autovettori. Infine, per ottenere i versori dagli autovettori, ci baster√† dividerli per la loro norma $L^2$, in modo da ottenere dei vettori di lunghezza unitaria.

---

### 4Ô∏è‚É£ **Selezione dei componenti principali**
Ordiniamo gli autovalori in ordine decrescente:
$$
\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d
$$
Selezioniamo i primi $k$ componenti principali che spiegano la maggior parte della varianza, cio√®:
$$
\text{Varianza spiegata} = \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^d \lambda_j}
$$

---

### 5Ô∏è‚É£ **Proiezione dei dati**
Proiettiamo il dataset standardizzato $Z$ nello spazio dei $k$ componenti principali selezionati. La matrice proiettata √® definita come:

$$
Z_{\text{PCA}} = Z V_k
$$

Dove:
- $Z$ √® la matrice del dataset standardizzato di dimensione $N \times d$ (con $N$ osservazioni e $p$ variabili),
- $V_k$ √® una matrice $d \times k$, i cui $k$ autovettori (o "vettori propri") corrispondono ai $k$ autovalori maggiori della matrice di covarianza.

#### üîç **Perch√© questa operazione funziona?**
La proiezione $Z V_k$ riduce la dimensione del dataset da $d$ a $k$ variabili, ma conserva la maggior parte dell'informazione originale (ovvero la varianza). Questo avviene per i seguenti motivi:

1. **Gli autovettori definiscono una nuova base**:
   Gli autovettori di $Z$ (calcolati dalla matrice di covarianza o di correlazione) rappresentano le direzioni principali lungo cui i dati variano maggiormente. In termini geometrici, queste direzioni sono ortogonali tra loro e formano una nuova base dello spazio originale.

2. **Proiezione lungo le direzioni principali**:
   Molte variabili del dataset originale possono essere fortemente correlate tra loro, il che significa che i dati "occupano" solo una porzione ridotta dello spazio ad alta dimensionalit√†. Gli autovettori identificano le direzioni in cui i dati sono pi√π sparsi. Proiettando $Z$ sugli autovettori principali, ci "spostiamo" dal sistema di coordinate originale a uno nuovo, allineato con queste direzioni principali.

3. **Selezione dei $k$ componenti principali**:
   Scegliendo i primi $k$ autovettori (che corrispondono ai $k$ autovalori pi√π grandi), ci limitiamo a considerare le direzioni che catturano la maggior parte della varianza nei dati. Le altre direzioni (con varianza pi√π piccola) vengono ignorate perch√© contribuiscono meno all'informazione complessiva.

#### üìè **Cosa succede a livello geometrico?**

- Il dataset standardizzato $Z$ √® rappresentato come un insieme di punti in uno spazio $d$-dimensionale.
- Gli autovettori $V_k$ definiscono un sotto-spazio $k$-dimensionale in cui i dati sono "schiacciati" (proiettati).
- La proiezione $Z V_k$ consiste nel prendere i punti originali e calcolare le loro coordinate rispetto a questo nuovo sistema di riferimento ridotto. Geometricamente, stiamo spostando i dati su un "piano" o "sottospazio" che meglio rappresenta la struttura del dataset.

Il risultato della proiezione, $Z_{\text{PCA}}$, √® un dataset ridotto di dimensione $N \times k$. Questo dataset:
- Mantiene la maggior parte della varianza originale (se $k$ √® scelto correttamente),
- Elimina la ridondanza e il rumore presente nelle dimensioni con varianza bassa,
- √à pi√π compatto e facilmente interpretabile rispetto al dataset originale.

---

## üîó Vantaggi

La PCA offre numerosi vantaggi, sia in termini di efficienza che di qualit√† dei risultati in analisi dei dati e machine learning. Di seguito, ogni vantaggio √® spiegato in dettaglio:

### 1Ô∏è‚É£ **Riduzione del rumore**
- **Cosa significa**: Durante il processo di PCA, le dimensioni del dataset associate a una varianza bassa (spesso causate da rumore o variabili irrilevanti) vengono scartate. Questo aiuta a concentrarsi sulle componenti che rappresentano la struttura "vera" dei dati.
- **Perch√© √® importante**: I dati reali spesso contengono rumore dovuto a misurazioni imprecise, errori di raccolta dati o informazioni non significative. Rimuovendo queste dimensioni, il dataset risultante √® pi√π pulito e meno soggetto a interpretazioni errate.

---

### 2Ô∏è‚É£ **Efficienza computazionale**
- **Cosa significa**: La riduzione del numero di dimensioni del dataset porta a una significativa riduzione dei costi computazionali, sia per il calcolo che per il successivo utilizzo nei modelli di machine learning.
- **Perch√© √® importante**:
  - Molti algoritmi di machine learning, come regressione logistica, SVM e reti neurali, funzionano pi√π velocemente quando il numero di feature √® minore.
  - Inoltre, la complessit√† computazionale degli algoritmi di ottimizzazione cresce con l'aumentare delle dimensioni del dataset.

---

### 3Ô∏è‚É£ **Rappresentazione compatta**
- **Cosa significa**: La PCA comprime le informazioni pi√π importanti del dataset originale in un numero inferiore di dimensioni, chiamate componenti principali. Questo consente una rappresentazione pi√π semplice e comprensibile dei dati.
- **Perch√© √® importante**:
  - Riducendo le dimensioni, il dataset diventa pi√π facile da esplorare e visualizzare. Ad esempio, possiamo rappresentare un dataset multidimensionale in 2D o 3D, facilitando la comprensione dei pattern nei dati.
  - Conserva la maggior parte della varianza (cio√® l‚Äôinformazione significativa) anche dopo la riduzione.

---

### 4Ô∏è‚É£ **Eliminazione della collinearit√†**
- **Cosa significa**: La PCA trasforma le variabili originali in componenti principali che sono ortogonali tra loro (non correlate). Questo risolve il problema della multi-collinearit√†, dove due o pi√π variabili sono fortemente correlate.
- **Perch√© √® importante**:
  - La multi-collinearit√† pu√≤ causare problemi in molti algoritmi, come regressione lineare, rendendo difficile stimare accuratamente i coefficienti.
  - Le componenti principali, essendo ortogonali, eliminano automaticamente questa problematica.
- **Esempio pratico**:
  - In un dataset economico, reddito e spese potrebbero essere fortemente correlati. La PCA pu√≤ combinare queste due variabili in una nuova componente principale che rappresenta il "livello economico" senza ridondanza.

---

### 5Ô∏è‚É£ **Facilita l'interpretazione e la visualizzazione dei dati**
- **Cosa significa**: Riducendo il numero di dimensioni, diventa pi√π semplice rappresentare i dati graficamente o interpretarne la struttura.
- **Perch√© √® importante**:
  - In dataset ad alta dimensionalit√†, √® difficile comprendere relazioni tra variabili o identificare pattern.
  - Dopo la PCA, possiamo spesso visualizzare i dati in 2D o 3D, rendendo pi√π intuitiva l'analisi esplorativa.

---

## ‚ö†Ô∏è Svantaggi
- **Perdita di interpretabilit√†**: I componenti principali non sono facili da interpretare.
- **Lineare**: La PCA √® limitata a relazioni lineari tra le variabili, quindi se i dati vivono in un sotto-spazio (i.e. [[Manifold Hypothesis|manifold]]) non lineare, la PCA pu√≤ non catturare tutta l'informazione. 
- **Sensibile alla scala**: I dati devono essere standardizzati.
- **Complessit√† computazionale**: La PCA √® abbastanza complessa dal punto di vista computazionale, perch√© richiede vari calcoli che coinvolgono matrici potenzialmente molto grandi. Quindi pu√≤ essere abbastanza costoso eseguire la PCA su dataset di grandi dimensioni.

---
## üöÄ **Conclusione**
Grazie a questi vantaggi, la PCA √® un'operazione fondamentale in molti ambiti dell'analisi dei dati, dalla riduzione delle dimensioni alla preparazione dei dati per modelli di machine learning. √à particolarmente utile quando si lavora con dataset ad alta dimensionalit√†, in cui √® cruciale ridurre la complessit√† senza perdere informazione significativa.